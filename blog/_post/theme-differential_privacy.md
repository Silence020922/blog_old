---
title: differential privacy
date: 2022-10-07
tag: note
sidebarDepth: 3
mathjax:
  presets: '\def\lr#1#2#3{\left#1#2\right#3}'
---
## 隐私？
什么是隐私这件事一直没有一个很好的定义。在我个人的理解里，需要针对个体的信息或者是某种属性。也就是说，一个群体所具备的某些属性并不能称之为隐私。     
举个例子，在通过统计分析后得到，在游戏中购买某类产品的人，更倾向于无目的的在折扣商品上进行消费。小A是如上所述的人。根据以上信息，我们只能推断出小A有可能倾向于购买折扣商品，但这只是表明了一种趋势，并没有暴露隐私。但如果我说，小A倾向于购买折扣商品，这属于隐私泄露。
## 背景？
通常情况下，你的各种数据会被一些数据中心或者是部分公司所掌握，在这种情况下，如果这些信息得不到利用，那信息在手里就是废物。且有时由于各种利益阶层的需要，一些信息需要被放到社会面，交由各个领域“专家”进行问诊。在过程中，免不得信息被adversary劫走。在这种情况下，对于个人自然不希望自己的信息遭到泄露，而对于信息的持有者，自然也不希望烂在手里，一无是处。于是隐私保护便提上日程了。    
最坏的情况自然是adversary可以了解到除你以外信息群中所有信息（如果他已经对所有信息都了如指掌，也就没有防范的必要了）。例如在一个班级里，adversary已知总共有20个单身汉，且除小B之外单身汉有19个。自然，他就可以得到一些奇怪的知识，而小B的信息在无形之中便被泄露了出去。想要避免这种情况发生，那便要让该组信息在含有小B和不包含小B时表现的数据差不多，即无论是否含有小B，该组单身信息都会显示19.5左右（当然，只是在概率上）。    
## 保护隐私？
>一种原理是淡化个体标志化信息，也就是常说的匿名化处理。基本思路如将身份证、姓名等信息直接删除；通过扰动和泛化的方式使得一个身份证号对应多个样本；将数据集纵向分割后按需公布等。此类操作隐私保护程度低，且在部分问题会遇到N-P hard问题。

>另一种思路是利用密码学手段加密解密，保护隐私。显然这类方法计算复杂度太高。同样面临的问题是为了便于分析加密机制的复杂程度，加密机制我们假定公开，很多情况下，adversary甚至能够了解到加密细节。

现将就这两种思路分别举例进行简单介绍……

# A
## 让数据学会说谎？    
举个二值问题的例子，吃香菜vs不吃香菜。若做以下规定，在做统计时抛一枚硬币，若正面向上，则如实记录，否则，再抛一枚硬币，若正面向上，记吃香菜，反之，记不吃香菜。    
首先，这组数据在样本数量足够多的前提下将具有统计意义。不妨设最终吃香菜的概率为a，而假定人群吃香菜的比率为p。自然有 **a=0.5*p+0.25** ,此时，在对方得到相应信息后可进行反解，得到目标数据p。同时，在这种情况下，即使adversary了解到操作细节，统计结果中，吃香菜的样本也并不能真切确定其真实吃香菜。简单计算如下：   
 >设事件A为统计结果中吃香菜、事件B为真实吃香菜    
 >显然，我们需要求的是P(B|A),利用贝叶斯公式我们可以得到    
 >P(B|A) = P(A|B)*P(A)/P(B)    
 >P(A|B) = 0.75; P(A) = 0.5p+0.25; P(B) = p    
 ## 差分隐私？
 通俗讲便是向统计结果中加入噪声，即引入随机性。直接结果便是导致数据集不准确，前提是保证数据集仍能通过统计分析的方法得到其总体趋势信息，目的是防止差分攻击，通常做法是加入服从拉普拉斯分布的噪声。
 ### 相邻数据集
 如果两个数据集只相差一条记录，那么这两个数据集是“相邻数据集”。在这基础上，如果对于相邻数据集的查询结果相近，那么那相差的一条记录的隐私就得到了保护。显然，如果对相邻数据集的查询结果越像，那么隐私保护力度越大。
 ### 数学描述
 设有两个数据集分别为D和D'，将D和D'中共有的记录从D和D'中删除，然后将D和D'合并所形成的新的数据集成为D和D'的对称差，记做D△D'。|D△D'|表示D△D'中记录的数量。
现有两个数据集D和D'，它们满足|D△D'|=1，M为一随机化算法，rang(M)表示算法M的所有可能的输出构成的集合，S是rang(M)的任一子集。如果算法M满足 
$\frac{P[S \in D]}{P[S \in D']} \leq e^{\epsilon}$
 ,那么我们则该算法满足ε-差分隐私，其中P为概率。
![](https://ask.qcloudimg.com/http-save/yehe-1268449/iiahnqe1z8.jpeg?imageView2/2/w/1620)
### 主要原理
如果对一些基础理论不太了解的话，可以看一下[这篇帖子](https://zhuanlan.zhihu.com/p/95687720)    
首先我们得目的是使得两种状态足够接近，自然利用KL-Divergence：    
$D(Y||Z) = E_{y~Y}[ln(\frac{Pr[Y=y]}{Pr[Z = y]})]$    
但我们不关系平均走势，我们只要在最大值点能有一个界进行限制，于是    
$D_{∞}(Y||Z) = \underset{S \subset Supp(Y)}{\max}[ln \frac{Pr[Y \in S]}{Pr[Z \in S ]}] = \underset{y \in Y}{\max}[ln \frac{Pr[Y = y]}{Pr[Z = y]}] \leq \epsilon$
化简过后，即可得到    
$Pr[M(x) \in S] \leq e^{\epsilon}Pr[M(x') \in S]$    
 当然，在实际得应用中为了增加算法得实用性，需要对差分隐私降低一些要求。    
$Pr[M(x)\in S]\leq e^{\epsilon}Pr[M(x')\in S] + \delta$     
从KL-Divergence角度来看，依然是    
$D^{\delta}_{∞}(Y||Z)=\underset {S \subset Supp(Y);Pr[Y \in S] \geq \delta}{\max}[ln \frac{Pr[Y \in S] - \delta}{Pr[Z \in S]}] \leq \epsilon$    
相较于前者，在分母上添加了一个较小得差距δ。    
### 实例演练
例如在一些领域得机器学习中，防止机器学习算法记忆个体的敏感信息，具体实例参考[这里](https://github.com/tensorflow/privacy)    
如果想在这方面有更多的了解，放一个过时[网页](http://www.cleverhans.io/privacy/2019/03/26/machine-learning-with-differential-privacy-in-tensorflow.html)供参考
# B
## 基于图边缘着色理论的隐私保护
待续...    
这方面的内容在此主题下并非重要，且本人对此并无什么了，随便一写图一乐    
当然，前提假定用户所面向的多个服务器未合谋，否则在合谋的前提下将更改加密模式    
### 举例
用户A希望从服务器上下载某文件，同时不希望某服务器对其行为有所记录，则可以采取以下操作    
>1. 在服务器上选取包含该文件在内的n个文件，并随机生成一个n维向量v，其每一分量唯一对应一个文件    
>2. 所需文件对应分量+1,生成向量v'    
>3. 将v发送至服务器A,同时将v'发送至服务器B    
如此，满足，单个服务器无法窃取我们的具体命令，且v'-v可发出明确信号。
## 小注
在写这篇博客时遇到了一个问题，，Markdown Math + Markdown Preview Enhance可实现本地预览完美显示数学公式，但对于如何上传到网页尚不明确。    
而解决的方案是在服务器上下载一个小插件[vuepress-plugin-mathjax](https://vuepress-community.netlify.app/zh/plugins/mathjax/)

>安装
>>npm i vuepress-plugin-mathjax OR yarn add vuepress-plugin-mathjax 
>
>再对*.vuepress/config.js添加相应命令。    
>
>>module.exports = {    
>>  plugins: [    
>>    [    
>>      'vuepress-plugin-mathjax',    
>>      {    
>>        target: 'svg',     
>>        macros: {    
>>          '*': '\\times',    
>>        },    
>>      },    
>>    ],    
>>  ],    
>>}    

